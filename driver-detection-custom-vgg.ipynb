{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfrom keras import layers\nfrom keras import models\nfrom keras.utils import to_categorical\nimport matplotlib.pyplot as plt\nfrom os import listdir, makedirs\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom keras.applications import VGG16, ResNet50, VGG19, InceptionV3, MobileNetV2\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Dropout, Conv2D, Activation, MaxPooling2D, BatchNormalization\nfrom keras import optimizers, regularizers\nfrom keras.optimizers import SGD\nfrom glob import glob\nimport cv2\nimport glob\nfrom keras import backend as K\nimport numpy as np \nimport pandas as pd \nimport os\nfrom keras import layers\nfrom keras import models, Sequential\nfrom keras.utils import to_categorical\nimport matplotlib.pyplot as plt\nfrom os import listdir, makedirs\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications import VGG16, ResNet50, VGG19, InceptionV3\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Dropout\nfrom keras.preprocessing.image import load_img\nfrom keras import optimizers, regularizers\nfrom keras.optimizers import SGD\nfrom glob import glob\nimport cv2\nfrom keras.callbacks import EarlyStopping, Callback\nfrom keras.preprocessing import image\nfrom keras.applications.inception_v3 import preprocess_input\nfrom keras.applications.imagenet_utils import preprocess_input, decode_predictions\nfrom keras.models import load_model\nimport h5py\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\ndata_dir = '../input/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_path='../input/state-farm-distracted-driver-detection/imgs/train/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RESOLUTION = 150\nBATCH_SIZE=128\n\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.3,)\nval_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.3)\n\ntrain_generator = train_datagen.flow_from_directory(\n        train_path,\n        target_size=(224, 224),\n        batch_size=BATCH_SIZE,\n        class_mode='categorical', subset=\"training\", color_mode='grayscale')\n\nval_generator = val_datagen.flow_from_directory(\n        train_path,\n        target_size=(224, 224),\n        batch_size=BATCH_SIZE,\n        class_mode='categorical', subset=\"validation\",  color_mode='grayscale')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_image(path):\n    image = cv2.imread(path, cv2.IMREAD_COLOR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return image\n\nlabels = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n\ncol = {'c0': 'safe driving',\n'c1': 'texting - right',\n'c2': 'talking on the phone - right',\n'c3': 'texting - left',\n'c4': 'talking on the phone - left',\n'c5':'operating the radio',\n'c6': 'drinking',\n'c7': 'reaching behind',\n'c8': 'hair and makeup',\n'c9': 'talking to passenger'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for label in labels:\n    f, ax = plt.subplots(figsize=(12, 10))\n    files = glob('{}/state-farm-distracted-driver-detection/imgs/train/{}/*.jpg'.format(data_dir, label))\n    \n    print('//t{} : {}'.format(label, col[label]))\n    for x in range(3):\n        plt.subplot(3, 3, x+1)\n        image = read_image(files[x])\n        plt.imshow(image)\n        plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    model = Sequential()\n    model.add(Conv2D(input_shape=(224,224,1),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\",trainable=False))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\",trainable=False))\n    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\",trainable=False))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\",trainable=False))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\",trainable=False))\n    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n    model.add(BatchNormalization())\n    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\",trainable=False))\n    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\",trainable=False))\n    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\",trainable=False))\n    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.25))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Flatten())\n    model.add(Dense(64, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.33))\n    model.add(Dense(32, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dense(10, activation = 'softmax')) \n    model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n    return model\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=create_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, filepath):\n    \n    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n    callbacks_list = [checkpoint]\n    \n    n_train = 15702\n    batch_size = 300\n    n_valid = 6722\n    history = model.fit_generator(\n           train_generator,\n           steps_per_epoch=n_train//batch_size,\n           epochs=25,\n           validation_data=val_generator,\n           validation_steps=n_valid//batch_size,  callbacks=callbacks_list)\n    \n    \n    # Plot\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs = range(1, len(acc) + 1)\n    plt.plot(epochs, acc, 'bo', label='Training acc')\n    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.figure()\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n    plt.show()\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = train(model, \"weights_best.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nmodel.save('model.h5')\n\ntf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True)\n\nimage_path = \"../input/state-farm-distracted-driver-detection/imgs/train/c5/img_10000.jpg\"\nimage = load_img(path=image_path, color_mode=\"grayscale\",\n                                              target_size=(224, 224))\ninput_arr = img_to_array(image)\ninput_arr = np.array([input_arr])  # Convert single image to a batch.\npredictions = model.predict(input_arr)\nprint(col[labels[np.argmax(predictions[0])]])\n\n#col\nimage = read_image(image_path)\nplt.imshow(image)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels[np.argmax(predictions[0])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}